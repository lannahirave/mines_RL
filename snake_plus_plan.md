# üêç Snake+ : –î–µ—Ç–∞–ª—å–Ω–∏–π –ø–ª–∞–Ω —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó

## –û–≥–ª—è–¥ –ø—Ä–æ–µ–∫—Ç—É

**–ú–µ—Ç–∞:** –°—Ç–≤–æ—Ä–∏—Ç–∏ –≥—Ä—É Snake –∑ –æ—Å–æ–±–ª–∏–≤–∏–º–∏ –æ–±'—î–∫—Ç–∞–º–∏ —Ç–∞ –Ω–∞–≤—á–∏—Ç–∏ RL-–∞–≥–µ–Ω—Ç–∞ –≥—Ä–∞—Ç–∏ –≤ –Ω–µ—ó, –¥–æ—Å–ª—ñ–¥–∂—É—é—á–∏ –≤–ø–ª–∏–≤ –¥–∏—Å–∫–æ–Ω—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó.

**–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó:** Python 3.10+, Gymnasium, PyTorch, Pygame, NumPy, Matplotlib

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É

```
snake_plus/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îÇ
‚îú‚îÄ‚îÄ env/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ snake_env.py          # Gymnasium —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
‚îÇ   ‚îú‚îÄ‚îÄ game_objects.py       # –ö–ª–∞—Å–∏ –æ–±'—î–∫—Ç—ñ–≤ (—è–±–ª—É–∫–∞, –æ—Ç—Ä—É—Ç–∞ —Ç–æ—â–æ)
‚îÇ   ‚îú‚îÄ‚îÄ snake.py              # –ö–ª–∞—Å –∑–º—ñ–π–∫–∏
‚îÇ   ‚îî‚îÄ‚îÄ renderer.py           # Pygame –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
‚îÇ
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ q_table_agent.py      # –¢–∞–±–ª–∏—á–Ω–∏–π Q-learning –∞–≥–µ–Ω—Ç
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py          # Deep Q-Network –∞–≥–µ–Ω—Ç
‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer.py      # Experience replay –±—É—Ñ–µ—Ä
‚îÇ   ‚îî‚îÄ‚îÄ networks.py           # –ù–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ –¥–ª—è DQN
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ train_q_table.py      # –°–∫—Ä–∏–ø—Ç –Ω–∞–≤—á–∞–Ω–Ω—è Q-table
‚îÇ   ‚îú‚îÄ‚îÄ train_dqn.py          # –°–∫—Ä–∏–ø—Ç –Ω–∞–≤—á–∞–Ω–Ω—è DQN
‚îÇ   ‚îî‚îÄ‚îÄ callbacks.py          # Callbacks –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ discount_analysis.py  # –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É Œ≥
‚îÇ   ‚îú‚îÄ‚îÄ compare_strategies.py # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
‚îÇ   ‚îî‚îÄ‚îÄ multi_agent.py        # –ë–∞–≥–∞—Ç–æ–∞–≥–µ–Ω—Ç–Ω—ñ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏
‚îÇ
‚îú‚îÄ‚îÄ visualization/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ plots.py              # –ì—Ä–∞—Ñ—ñ–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
‚îÇ   ‚îú‚îÄ‚îÄ game_recorder.py      # –ó–∞–ø–∏—Å –≤—ñ–¥–µ–æ –≥—Ä–∏
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py          # –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π –¥–∞—à–±–æ—Ä–¥
‚îÇ
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ default_env.yaml      # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
‚îÇ   ‚îú‚îÄ‚îÄ training.yaml         # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
‚îÇ   ‚îî‚îÄ‚îÄ experiments.yaml      # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_env.py
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ test_game_logic.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ analysis.ipynb        # Jupyter –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
‚îÇ
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ models/               # –ó–±–µ—Ä–µ–∂–µ–Ω—ñ –º–æ–¥–µ–ª—ñ
    ‚îú‚îÄ‚îÄ logs/                 # –õ–æ–≥–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    ‚îî‚îÄ‚îÄ plots/                # –ó–±–µ—Ä–µ–∂–µ–Ω—ñ –≥—Ä–∞—Ñ—ñ–∫–∏
```

---

## üì¶ –ó–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ (requirements.txt)

```
gymnasium>=0.29.0
pygame>=2.5.0
numpy>=1.24.0
torch>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
pyyaml>=6.0
tensorboard>=2.13.0
tqdm>=4.65.0
pytest>=7.3.0
imageio>=2.31.0
```

---

## üéÆ –ß–ê–°–¢–ò–ù–ê 1: –°–µ—Ä–µ–¥–æ–≤–∏—â–µ (env/)

### 1.1 game_objects.py

```python
"""
–ú–æ–¥—É–ª—å –∑ –∫–ª–∞—Å–∞–º–∏ —ñ–≥—Ä–æ–≤–∏—Ö –æ–±'—î–∫—Ç—ñ–≤.
"""

from enum import Enum, auto
from dataclasses import dataclass
from typing import Tuple
import random

class ObjectType(Enum):
    """–¢–∏–ø–∏ –æ–±'—î–∫—Ç—ñ–≤ –Ω–∞ –ø–æ–ª—ñ."""
    APPLE = auto()       # –ó–≤–∏—á–∞–π–Ω–µ —è–±–ª—É–∫–æ: +1 –¥–æ–≤–∂–∏–Ω–∞, +10 –æ—á–æ–∫
    GOLDEN = auto()      # –ó–æ–ª–æ—Ç–µ: +3 –¥–æ–≤–∂–∏–Ω–∞, +30-70 –æ—á–æ–∫
    POISON = auto()      # –û—Ç—Ä—É—Ç–∞: —Å–º–µ—Ä—Ç—å
    SOUR = auto()        # –ö–∏—Å–ª–µ: -1...-3 –¥–æ–≤–∂–∏–Ω–∞, -5 –æ—á–æ–∫
    ROTTEN = auto()      # –ì–Ω–∏–ª–µ: –≤—ñ–¥—Ä–∏–≤–∞—î —Ö–≤—ñ—Å—Ç ‚Üí –ø–µ—Ä–µ—à–∫–æ–¥–∞, -20 –æ—á–æ–∫
    OBSTACLE = auto()    # –ü–µ—Ä–µ—à–∫–æ–¥–∞ (–≤—ñ–¥—ñ—Ä–≤–∞–Ω–∏–π —Ö–≤—ñ—Å—Ç)

@dataclass
class GameObject:
    """–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å —ñ–≥—Ä–æ–≤–æ–≥–æ –æ–±'—î–∫—Ç–∞."""
    x: int
    y: int
    object_type: ObjectType
    lifetime: int = -1  # -1 = –≤—ñ—á–Ω–∏–π, —ñ–Ω–∞–∫—à–µ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ –¥–æ –∑–Ω–∏–∫–Ω–µ–Ω–Ω—è
    
    @property
    def position(self) -> Tuple[int, int]:
        return (self.x, self.y)

class ObjectFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–±'—î–∫—Ç—ñ–≤ –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    
    def __init__(self, spawn_probs: dict, grid_size: Tuple[int, int]):
        """
        Args:
            spawn_probs: {"apple": 0.5, "golden": 0.1, ...}
            grid_size: (width, height)
        """
        self.spawn_probs = spawn_probs
        self.grid_size = grid_size
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
        total = sum(spawn_probs.values())
        self.normalized_probs = {k: v/total for k, v in spawn_probs.items()}
    
    def create_random_object(self, occupied_positions: set) -> GameObject:
        """
        –°—Ç–≤–æ—Ä—é—î –≤–∏–ø–∞–¥–∫–æ–≤–∏–π –æ–±'—î–∫—Ç —É –≤—ñ–ª—å–Ω—ñ–π –ø–æ–∑–∏—Ü—ñ—ó.
        
        Args:
            occupied_positions: –º–Ω–æ–∂–∏–Ω–∞ –∑–∞–π–Ω—è—Ç–∏—Ö –ø–æ–∑–∏—Ü—ñ–π {(x, y), ...}
        
        Returns:
            GameObject –∞–±–æ None —è–∫—â–æ –Ω–µ–º–∞—î –º—ñ—Å—Ü—è
        """
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–ª—å–Ω—É –ø–æ–∑–∏—Ü—ñ—é
        free_positions = [
            (x, y) 
            for x in range(self.grid_size[0]) 
            for y in range(self.grid_size[1])
            if (x, y) not in occupied_positions
        ]
        
        if not free_positions:
            return None
        
        x, y = random.choice(free_positions)
        
        # –í–∏–±–∏—Ä–∞—î–º–æ —Ç–∏–ø –æ–±'—î–∫—Ç–∞
        obj_type = self._random_type()
        
        return GameObject(x=x, y=y, object_type=obj_type)
    
    def _random_type(self) -> ObjectType:
        """–í–∏–±–∏—Ä–∞—î –≤–∏–ø–∞–¥–∫–æ–≤–∏–π —Ç–∏–ø –∑–≥—ñ–¥–Ω–æ –∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏."""
        r = random.random()
        cumulative = 0
        
        type_mapping = {
            "apple": ObjectType.APPLE,
            "golden": ObjectType.GOLDEN,
            "poison": ObjectType.POISON,
            "sour": ObjectType.SOUR,
            "rotten": ObjectType.ROTTEN,
        }
        
        for name, prob in self.normalized_probs.items():
            cumulative += prob
            if r <= cumulative:
                return type_mapping[name]
        
        return ObjectType.APPLE  # fallback

class RewardCalculator:
    """–û–±—á–∏—Å–ª—é—î –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏ –∑–∞ —Ä—ñ–∑–Ω—ñ –ø–æ–¥—ñ—ó."""
    
    # –ë–∞–∑–æ–≤—ñ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏
    REWARDS = {
        ObjectType.APPLE: 10,
        ObjectType.GOLDEN: (30, 70),  # –≤–∏–ø–∞–¥–∫–æ–≤–æ –≤ –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ
        ObjectType.POISON: -1000,
        ObjectType.SOUR: -5,
        ObjectType.ROTTEN: -20,
    }
    
    # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∏
    DEATH_PENALTY = -1000
    STEP_PENALTY = -0.1      # —à—Ç—Ä–∞—Ñ –∑–∞ –∫–æ–∂–µ–Ω –∫—Ä–æ–∫ (—Å—Ç–∏–º—É–ª—é—î –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å)
    SURVIVAL_BONUS = 0.5     # –±–æ–Ω—É—Å –∑–∞ –≤–∏–∂–∏–≤–∞–Ω–Ω—è
    
    @classmethod
    def get_reward(cls, obj_type: ObjectType) -> float:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É –∑–∞ –∑'—ó–¥–∞–Ω–Ω—è –æ–±'—î–∫—Ç–∞."""
        reward = cls.REWARDS.get(obj_type, 0)
        
        if isinstance(reward, tuple):
            return random.uniform(reward[0], reward[1])
        
        return reward
    
    @classmethod
    def get_length_change(cls, obj_type: ObjectType) -> int:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –∑–º—ñ–Ω—É –¥–æ–≤–∂–∏–Ω–∏ –∑–º—ñ–π–∫–∏."""
        changes = {
            ObjectType.APPLE: 1,
            ObjectType.GOLDEN: 3,
            ObjectType.POISON: 0,  # —Å–º–µ—Ä—Ç—å, –Ω–µ –≤–∞–∂–ª–∏–≤–æ
            ObjectType.SOUR: -random.randint(1, 3),  # –≤–∏–ø–∞–¥–∫–æ–≤–æ -1...-3
            ObjectType.ROTTEN: 0,  # –æ–±—Ä–æ–±–ª—è—î—Ç—å—Å—è –æ–∫—Ä–µ–º–æ (–≤—ñ–¥—Ä–∏–≤ —Ö–≤–æ—Å—Ç–∞)
        }
        return changes.get(obj_type, 0)
```

### 1.2 snake.py

```python
"""
–ú–æ–¥—É–ª—å –∑ –∫–ª–∞—Å–æ–º –∑–º—ñ–π–∫–∏.
"""

from enum import Enum, auto
from typing import List, Tuple, Optional
from collections import deque

class Direction(Enum):
    """–ù–∞–ø—Ä—è–º–∫–∏ —Ä—É—Ö—É."""
    UP = (0, -1)
    DOWN = (0, 1)
    LEFT = (-1, 0)
    RIGHT = (1, 0)

class Action(Enum):
    """–î—ñ—ó –∞–≥–µ–Ω—Ç–∞ (–≤—ñ–¥–Ω–æ—Å–Ω—ñ)."""
    FORWARD = 0     # –†—É—Ö–∞—Ç–∏—Å—å –ø—Ä—è–º–æ
    TURN_LEFT = 1   # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –ª—ñ–≤–æ—Ä—É—á
    TURN_RIGHT = 2  # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –ø—Ä–∞–≤–æ—Ä—É—á

class Snake:
    """–ö–ª–∞—Å –∑–º—ñ–π–∫–∏."""
    
    # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ–≤–æ—Ä–æ—Ç—É: –ø–æ—Ç–æ—á–Ω–∏–π –Ω–∞–ø—Ä—è–º–æ–∫ ‚Üí –Ω–æ–≤–∏–π –Ω–∞–ø—Ä—è–º–æ–∫ –ø—Ä–∏ –ø–æ–≤–æ—Ä–æ—Ç—ñ
    TURN_LEFT_MAP = {
        Direction.UP: Direction.LEFT,
        Direction.LEFT: Direction.DOWN,
        Direction.DOWN: Direction.RIGHT,
        Direction.RIGHT: Direction.UP,
    }
    
    TURN_RIGHT_MAP = {
        Direction.UP: Direction.RIGHT,
        Direction.RIGHT: Direction.DOWN,
        Direction.DOWN: Direction.LEFT,
        Direction.LEFT: Direction.UP,
    }
    
    def __init__(self, start_pos: Tuple[int, int], start_length: int = 3, 
                 start_direction: Direction = Direction.RIGHT):
        """
        Args:
            start_pos: –ø–æ—á–∞—Ç–∫–æ–≤–∞ –ø–æ–∑–∏—Ü—ñ—è –≥–æ–ª–æ–≤–∏ (x, y)
            start_length: –ø–æ—á–∞—Ç–∫–æ–≤–∞ –¥–æ–≤–∂–∏–Ω–∞
            start_direction: –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –Ω–∞–ø—Ä—è–º–æ–∫
        """
        self.direction = start_direction
        self.grow_pending = 0  # —Å–∫—ñ–ª—å–∫–∏ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥–æ–¥–∞—Ç–∏
        
        # –¢—ñ–ª–æ —è–∫ deque: [–≥–æ–ª–æ–≤–∞, ..., —Ö–≤—ñ—Å—Ç]
        self.body: deque = deque()
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —Ç—ñ–ª–æ
        x, y = start_pos
        dx, dy = start_direction.value
        
        for i in range(start_length):
            self.body.append((x - i * dx, y - i * dy))
    
    @property
    def head(self) -> Tuple[int, int]:
        """–ü–æ–∑–∏—Ü—ñ—è –≥–æ–ª–æ–≤–∏."""
        return self.body[0]
    
    @property
    def tail(self) -> Tuple[int, int]:
        """–ü–æ–∑–∏—Ü—ñ—è —Ö–≤–æ—Å—Ç–∞."""
        return self.body[-1]
    
    @property
    def length(self) -> int:
        """–î–æ–≤–∂–∏–Ω–∞ –∑–º—ñ–π–∫–∏."""
        return len(self.body)
    
    def get_body_set(self) -> set:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –º–Ω–æ–∂–∏–Ω—É –ø–æ–∑–∏—Ü—ñ–π —Ç—ñ–ª–∞ (–¥–ª—è —à–≤–∏–¥–∫–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∫–æ–ª—ñ–∑—ñ–π)."""
        return set(self.body)
    
    def apply_action(self, action: Action) -> None:
        """–ó–º—ñ–Ω—é—î –Ω–∞–ø—Ä—è–º–æ–∫ –∑–≥—ñ–¥–Ω–æ –∑ –¥—ñ—î—é."""
        if action == Action.TURN_LEFT:
            self.direction = self.TURN_LEFT_MAP[self.direction]
        elif action == Action.TURN_RIGHT:
            self.direction = self.TURN_RIGHT_MAP[self.direction]
        # FORWARD - –Ω–∞–ø—Ä—è–º–æ–∫ –Ω–µ –∑–º—ñ–Ω—é—î—Ç—å—Å—è
    
    def move(self) -> Tuple[int, int]:
        """
        –†—É—Ö–∞—î –∑–º—ñ–π–∫—É –Ω–∞ –æ–¥–∏–Ω –∫—Ä–æ–∫.
        
        Returns:
            –ù–æ–≤–∞ –ø–æ–∑–∏—Ü—ñ—è –≥–æ–ª–æ–≤–∏
        """
        # –û–±—á–∏—Å–ª—é—î–º–æ –Ω–æ–≤—É –ø–æ–∑–∏—Ü—ñ—é –≥–æ–ª–æ–≤–∏
        hx, hy = self.head
        dx, dy = self.direction.value
        new_head = (hx + dx, hy + dy)
        
        # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—É –≥–æ–ª–æ–≤—É
        self.body.appendleft(new_head)
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ö–≤—ñ—Å—Ç (—è–∫—â–æ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Ä–æ—Å—Ç–∏)
        if self.grow_pending > 0:
            self.grow_pending -= 1
        else:
            self.body.pop()
        
        return new_head
    
    def grow(self, amount: int = 1) -> None:
        """–ó–±—ñ–ª—å—à—É—î –¥–æ–≤–∂–∏–Ω—É –Ω–∞ amount —Å–µ–≥–º–µ–Ω—Ç—ñ–≤."""
        self.grow_pending += amount
    
    def shrink(self, amount: int) -> None:
        """
        –ó–º–µ–Ω—à—É—î –¥–æ–≤–∂–∏–Ω—É –Ω–∞ amount —Å–µ–≥–º–µ–Ω—Ç—ñ–≤.
        –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ = 1 (—Ç—ñ–ª—å–∫–∏ –≥–æ–ª–æ–≤–∞).
        """
        for _ in range(min(amount, len(self.body) - 1)):
            self.body.pop()
    
    def detach_tail(self, amount: int) -> List[Tuple[int, int]]:
        """
        –í—ñ–¥—Ä–∏–≤–∞—î amount —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —Ö–≤–æ—Å—Ç–∞.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ–∑–∏—Ü—ñ–π –≤—ñ–¥—ñ—Ä–≤–∞–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ (—Å—Ç–∞–Ω—É—Ç—å –ø–µ—Ä–µ—à–∫–æ–¥–∞–º–∏)
        """
        detached = []
        for _ in range(min(amount, len(self.body) - 1)):
            pos = self.body.pop()
            detached.append(pos)
        return detached
    
    def check_self_collision(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –≥–æ–ª–æ–≤–∞ –∑—ñ—Ç–∫–Ω—É–ª–∞—Å—å –∑ —Ç—ñ–ª–æ–º."""
        return self.head in list(self.body)[1:]
```

### 1.3 snake_env.py

```python
"""
Gymnasium —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –¥–ª—è Snake+.
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Optional, Tuple, Dict, Any, List
import random

from .snake import Snake, Direction, Action
from .game_objects import (
    ObjectType, GameObject, ObjectFactory, RewardCalculator
)

class SnakePlusEnv(gym.Env):
    """
    Snake+ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ç–∏–ø–∞–º–∏ –æ–±'—î–∫—Ç—ñ–≤.
    
    –°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è (observation):
        –í–∞—Ä—ñ–∞–Ω—Ç 1 (–¥–ª—è Q-table): –≤–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫
        –í–∞—Ä—ñ–∞–Ω—Ç 2 (–¥–ª—è DQN): 2D –º–∞—Ç—Ä–∏—Ü—è —Å—Ç–∞–Ω—É –ø–æ–ª—è
    
    –î—ñ—ó (actions):
        0: –†—É—Ö–∞—Ç–∏—Å—å –ø—Ä—è–º–æ
        1: –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –ª—ñ–≤–æ—Ä—É—á
        2: –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –ø—Ä–∞–≤–æ—Ä—É—á
    
    –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∏:
        - –Ø–±–ª—É–∫–æ: +10
        - –ó–æ–ª–æ—Ç–µ: +30...+70 (–≤–∏–ø–∞–¥–∫–æ–≤–æ)
        - –û—Ç—Ä—É—Ç–∞: -1000 (—Å–º–µ—Ä—Ç—å)
        - –ö–∏—Å–ª–µ: -5
        - –ì–Ω–∏–ª–µ: -20
        - –ö–æ–∂–µ–Ω –∫—Ä–æ–∫: -0.1
        - –°–º–µ—Ä—Ç—å (—Å—Ç—ñ–Ω–∞/—Ç—ñ–ª–æ): -1000
    """
    
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 10}
    
    def __init__(
        self,
        grid_size: Tuple[int, int] = (15, 15),
        spawn_probs: Optional[Dict[str, float]] = None,
        max_objects: int = 5,
        obstacle_decay: Optional[int] = 50,
        max_steps: int = 1000,
        observation_type: str = "features",  # "features" –∞–±–æ "grid"
        render_mode: Optional[str] = None,
    ):
        """
        Args:
            grid_size: —Ä–æ–∑–º—ñ—Ä –ø–æ–ª—è (width, height)
            spawn_probs: –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø–æ—è–≤–∏ –æ–±'—î–∫—Ç—ñ–≤
            max_objects: –º–∞–∫—Å–∏–º—É–º –æ–±'—î–∫—Ç—ñ–≤ –Ω–∞ –ø–æ–ª—ñ (–±–µ–∑ –ø–µ—Ä–µ—à–∫–æ–¥)
            obstacle_decay: —á–µ—Ä–µ–∑ —Å–∫—ñ–ª—å–∫–∏ –∫—Ä–æ–∫—ñ–≤ –∑–Ω–∏–∫–∞—î –ø–µ—Ä–µ—à–∫–æ–¥–∞ (None = –Ω—ñ–∫–æ–ª–∏)
            max_steps: –º–∞–∫—Å–∏–º—É–º –∫—Ä–æ–∫—ñ–≤ –∑–∞ –µ–ø—ñ–∑–æ–¥
            observation_type: —Ç–∏–ø —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è
            render_mode: —Ä–µ–∂–∏–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥—É
        """
        super().__init__()
        
        self.grid_size = grid_size
        self.max_objects = max_objects
        self.obstacle_decay = obstacle_decay
        self.max_steps = max_steps
        self.observation_type = observation_type
        self.render_mode = render_mode
        
        # –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        if spawn_probs is None:
            spawn_probs = {
                "apple": 0.50,
                "golden": 0.10,
                "poison": 0.15,
                "sour": 0.15,
                "rotten": 0.10,
            }
        
        self.spawn_probs = spawn_probs
        self.object_factory = ObjectFactory(spawn_probs, grid_size)
        
        # –ü—Ä–æ—Å—Ç—ñ—Ä –¥—ñ–π: 3 –¥—ñ—ó
        self.action_space = spaces.Discrete(3)
        
        # –ü—Ä–æ—Å—Ç—ñ—Ä —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å
        if observation_type == "features":
            # –í–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫ –¥–ª—è Q-table
            # [danger_left, danger_straight, danger_right,  # 3
            #  dir_up, dir_down, dir_left, dir_right,       # 4
            #  food_up, food_down, food_left, food_right,   # 4
            #  nearest_obj_type (one-hot 5),                # 5
            #  distance_to_nearest_food,                    # 1
            #  snake_length_normalized]                     # 1
            # –í—Å—å–æ–≥–æ: 18 –æ–∑–Ω–∞–∫
            self.observation_space = spaces.Box(
                low=0, high=1, shape=(18,), dtype=np.float32
            )
        else:  # "grid"
            # 3D –º–∞—Ç—Ä–∏—Ü—è –¥–ª—è CNN
            # –ö–∞–Ω–∞–ª–∏: [–≥–æ–ª–æ–≤–∞, —Ç—ñ–ª–æ, —è–±–ª—É–∫–æ, –∑–æ–ª–æ—Ç–µ, –æ—Ç—Ä—É—Ç–∞, –∫–∏—Å–ª–µ, –≥–Ω–∏–ª–µ, –ø–µ—Ä–µ—à–∫–æ–¥–∞]
            self.observation_space = spaces.Box(
                low=0, high=1,
                shape=(8, grid_size[1], grid_size[0]),
                dtype=np.float32
            )
        
        # –°—Ç–∞–Ω –≥—Ä–∏ (—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è –≤ reset)
        self.snake: Optional[Snake] = None
        self.objects: List[GameObject] = []
        self.obstacles: List[GameObject] = []  # –≤—ñ–¥—ñ—Ä–≤–∞–Ω—ñ —Ö–≤–æ—Å—Ç–∏
        self.score: int = 0
        self.steps: int = 0
        
        # –†–µ–Ω–¥–µ—Ä–µ—Ä (—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É render)
        self.renderer = None
    
    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        –°–∫–∏–¥–∞—î —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –¥–æ –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ —Å—Ç–∞–Ω—É.
        
        Returns:
            observation: –ø–æ—á–∞—Ç–∫–æ–≤–µ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è
            info: –¥–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
        """
        super().reset(seed=seed)
        
        # –ü–æ—á–∞—Ç–∫–æ–≤–∞ –ø–æ–∑–∏—Ü—ñ—è –∑–º—ñ–π–∫–∏ (—Ü–µ–Ω—Ç—Ä –ø–æ–ª—è)
        start_x = self.grid_size[0] // 2
        start_y = self.grid_size[1] // 2
        
        self.snake = Snake(
            start_pos=(start_x, start_y),
            start_length=3,
            start_direction=Direction.RIGHT
        )
        
        self.objects = []
        self.obstacles = []
        self.score = 0
        self.steps = 0
        
        # –°–ø–∞–≤–Ω–∏–º–æ –ø–æ—á–∞—Ç–∫–æ–≤—ñ –æ–±'—î–∫—Ç–∏
        self._spawn_objects()
        
        observation = self._get_observation()
        info = self._get_info()
        
        return observation, info
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """
        –í–∏–∫–æ–Ω—É—î –æ–¥–∏–Ω –∫—Ä–æ–∫.
        
        Args:
            action: 0=–ø—Ä—è–º–æ, 1=–ª—ñ–≤–æ—Ä—É—á, 2=–ø—Ä–∞–≤–æ—Ä—É—á
        
        Returns:
            observation: –Ω–æ–≤–µ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è
            reward: –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
            terminated: —á–∏ –∑–∞–∫—ñ–Ω—á–∏–ª–∞—Å—å –≥—Ä–∞ (—Å–º–µ—Ä—Ç—å)
            truncated: —á–∏ –æ–±—Ä—ñ–∑–∞–Ω–æ (max_steps)
            info: –¥–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
        """
        self.steps += 1
        reward = RewardCalculator.STEP_PENALTY  # —à—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–æ–∫
        terminated = False
        truncated = False
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –¥—ñ—é
        self.snake.apply_action(Action(action))
        
        # –†—É—Ö–∞—î–º–æ –∑–º—ñ–π–∫—É
        new_head = self.snake.move()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–ª—ñ–∑—ñ—ó –∑—ñ —Å—Ç—ñ–Ω–∞–º–∏
        if not self._is_valid_position(new_head):
            reward += RewardCalculator.DEATH_PENALTY
            terminated = True
            return self._get_observation(), reward, terminated, truncated, self._get_info()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–ª—ñ–∑—ñ—é –∑ —Ç—ñ–ª–æ–º
        if self.snake.check_self_collision():
            reward += RewardCalculator.DEATH_PENALTY
            terminated = True
            return self._get_observation(), reward, terminated, truncated, self._get_info()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–ª—ñ–∑—ñ—é –∑ –ø–µ—Ä–µ—à–∫–æ–¥–∞–º–∏
        obstacle_positions = {obs.position for obs in self.obstacles}
        if new_head in obstacle_positions:
            reward += RewardCalculator.DEATH_PENALTY
            terminated = True
            return self._get_observation(), reward, terminated, truncated, self._get_info()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–ª—ñ–∑—ñ—é –∑ –æ–±'—î–∫—Ç–∞–º–∏
        eaten_object = None
        for obj in self.objects:
            if obj.position == new_head:
                eaten_object = obj
                break
        
        if eaten_object:
            reward += self._process_eaten_object(eaten_object)
            
            # –û—Ç—Ä—É—Ç–∞ = —Å–º–µ—Ä—Ç—å
            if eaten_object.object_type == ObjectType.POISON:
                terminated = True
                return self._get_observation(), reward, terminated, truncated, self._get_info()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –∑–º—ñ–π–∫–∞ –Ω–µ —Å—Ç–∞–ª–∞ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ—é
            if self.snake.length < 1:
                terminated = True
                return self._get_observation(), reward, terminated, truncated, self._get_info()
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –ø–µ—Ä–µ—à–∫–æ–¥–∏ (decay)
        self._update_obstacles()
        
        # –°–ø–∞–≤–Ω–∏–º–æ –Ω–æ–≤—ñ –æ–±'—î–∫—Ç–∏ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        self._spawn_objects()
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–∏–∂–∏–≤–∞–Ω–Ω—è
        reward += RewardCalculator.SURVIVAL_BONUS
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ max_steps
        if self.steps >= self.max_steps:
            truncated = True
        
        observation = self._get_observation()
        info = self._get_info()
        
        return observation, reward, terminated, truncated, info
    
    def _process_eaten_object(self, obj: GameObject) -> float:
        """
        –û–±—Ä–æ–±–ª—è—î –∑'—ó–¥–∞–Ω–Ω—è –æ–±'—î–∫—Ç–∞.
        
        Returns:
            –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞
        """
        reward = RewardCalculator.get_reward(obj.object_type)
        self.score += max(0, int(reward))
        
        # –í–∏–¥–∞–ª—è—î–º–æ –æ–±'—î–∫—Ç
        self.objects.remove(obj)
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –µ—Ñ–µ–∫—Ç
        if obj.object_type == ObjectType.APPLE:
            self.snake.grow(1)
        
        elif obj.object_type == ObjectType.GOLDEN:
            self.snake.grow(3)
        
        elif obj.object_type == ObjectType.SOUR:
            shrink_amount = random.randint(1, 3)
            self.snake.shrink(shrink_amount)
        
        elif obj.object_type == ObjectType.ROTTEN:
            # –í—ñ–¥—Ä–∏–≤–∞—î–º–æ 3-5 —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ —Ö–≤–æ—Å—Ç–∞
            detach_amount = random.randint(3, 5)
            detached_positions = self.snake.detach_tail(detach_amount)
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–µ—Ä–µ—à–∫–æ–¥–∏
            for pos in detached_positions:
                obstacle = GameObject(
                    x=pos[0], y=pos[1],
                    object_type=ObjectType.OBSTACLE,
                    lifetime=self.obstacle_decay if self.obstacle_decay else -1
                )
                self.obstacles.append(obstacle)
        
        return reward
    
    def _update_obstacles(self) -> None:
        """–û–Ω–æ–≤–ª—é—î —á–∞—Å –∂–∏—Ç—Ç—è –ø–µ—Ä–µ—à–∫–æ–¥, –≤–∏–¥–∞–ª—è—î —Å—Ç–∞—Ä—ñ."""
        if self.obstacle_decay is None:
            return
        
        remaining = []
        for obs in self.obstacles:
            if obs.lifetime > 0:
                obs.lifetime -= 1
                if obs.lifetime > 0:
                    remaining.append(obs)
            elif obs.lifetime == -1:  # –≤—ñ—á–Ω–∏–π
                remaining.append(obs)
        
        self.obstacles = remaining
    
    def _spawn_objects(self) -> None:
        """–°–ø–∞–≤–Ω–∏—Ç—å –æ–±'—î–∫—Ç–∏ –¥–æ max_objects."""
        occupied = self._get_occupied_positions()
        
        while len(self.objects) < self.max_objects:
            obj = self.object_factory.create_random_object(occupied)
            if obj is None:
                break  # –Ω–µ–º–∞—î –º—ñ—Å—Ü—è
            
            self.objects.append(obj)
            occupied.add(obj.position)
    
    def _get_occupied_positions(self) -> set:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ –∑–∞–π–Ω—è—Ç—ñ –ø–æ–∑–∏—Ü—ñ—ó."""
        occupied = self.snake.get_body_set()
        occupied.update(obj.position for obj in self.objects)
        occupied.update(obs.position for obs in self.obstacles)
        return occupied
    
    def _is_valid_position(self, pos: Tuple[int, int]) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –ø–æ–∑–∏—Ü—ñ—è –≤ –º–µ–∂–∞—Ö –ø–æ–ª—è."""
        x, y = pos
        return 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1]
    
    def _get_observation(self) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä—É—î —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è."""
        if self.observation_type == "features":
            return self._get_feature_observation()
        else:
            return self._get_grid_observation()
    
    def _get_feature_observation(self) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä—É—î –≤–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫ –¥–ª—è Q-table –∞–≥–µ–Ω—Ç–∞.
        
        –û–∑–Ω–∞–∫–∏:
        [0-2]: danger_left, danger_straight, danger_right
        [3-6]: direction one-hot (up, down, left, right)
        [7-10]: food direction (up, down, left, right)
        [11-15]: nearest object type one-hot (apple, golden, poison, sour, rotten)
        [16]: distance to nearest food (normalized)
        [17]: snake length (normalized)
        """
        features = np.zeros(18, dtype=np.float32)
        
        head = self.snake.head
        direction = self.snake.direction
        
        # –ù–µ–±–µ–∑–ø–µ–∫–∞ –≤ 3 –Ω–∞–ø—Ä—è–º–∫–∞—Ö (–≤—ñ–¥–Ω–æ—Å–Ω–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –Ω–∞–ø—Ä—è–º–∫—É)
        features[0] = self._is_danger(self._get_left_direction(direction))
        features[1] = self._is_danger(direction)  # –ø—Ä—è–º–æ
        features[2] = self._is_danger(self._get_right_direction(direction))
        
        # –ù–∞–ø—Ä—è–º–æ–∫ (one-hot)
        dir_idx = {Direction.UP: 3, Direction.DOWN: 4, 
                   Direction.LEFT: 5, Direction.RIGHT: 6}
        features[dir_idx[direction]] = 1
        
        # –ù–∞–ø—Ä—è–º–æ–∫ –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ—ó —ó–∂—ñ
        food_objects = [obj for obj in self.objects 
                       if obj.object_type in (ObjectType.APPLE, ObjectType.GOLDEN)]
        
        if food_objects:
            nearest_food = min(food_objects, 
                             key=lambda o: abs(o.x - head[0]) + abs(o.y - head[1]))
            
            # food_up, food_down, food_left, food_right
            features[7] = 1 if nearest_food.y < head[1] else 0
            features[8] = 1 if nearest_food.y > head[1] else 0
            features[9] = 1 if nearest_food.x < head[0] else 0
            features[10] = 1 if nearest_food.x > head[0] else 0
            
            # –í—ñ–¥—Å—Ç–∞–Ω—å (normalized)
            dist = abs(nearest_food.x - head[0]) + abs(nearest_food.y - head[1])
            max_dist = self.grid_size[0] + self.grid_size[1]
            features[16] = dist / max_dist
        
        # –ù–∞–π–±–ª–∏–∂—á–∏–π –æ–±'—î–∫—Ç –±—É–¥—å-—è–∫–æ–≥–æ —Ç–∏–ø—É
        if self.objects:
            nearest_obj = min(self.objects,
                            key=lambda o: abs(o.x - head[0]) + abs(o.y - head[1]))
            
            type_idx = {
                ObjectType.APPLE: 11,
                ObjectType.GOLDEN: 12,
                ObjectType.POISON: 13,
                ObjectType.SOUR: 14,
                ObjectType.ROTTEN: 15,
            }
            features[type_idx[nearest_obj.object_type]] = 1
        
        # –î–æ–≤–∂–∏–Ω–∞ –∑–º—ñ–π–∫–∏ (normalized)
        max_length = self.grid_size[0] * self.grid_size[1]
        features[17] = self.snake.length / max_length
        
        return features
    
    def _get_grid_observation(self) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä—É—î 3D –º–∞—Ç—Ä–∏—Ü—é –¥–ª—è CNN.
        
        –ö–∞–Ω–∞–ª–∏:
        0: –≥–æ–ª–æ–≤–∞
        1: —Ç—ñ–ª–æ
        2: —è–±–ª—É–∫–æ
        3: –∑–æ–ª–æ—Ç–µ
        4: –æ—Ç—Ä—É—Ç–∞
        5: –∫–∏—Å–ª–µ
        6: –≥–Ω–∏–ª–µ
        7: –ø–µ—Ä–µ—à–∫–æ–¥–∞
        """
        grid = np.zeros((8, self.grid_size[1], self.grid_size[0]), dtype=np.float32)
        
        # –ì–æ–ª–æ–≤–∞
        hx, hy = self.snake.head
        grid[0, hy, hx] = 1
        
        # –¢—ñ–ª–æ
        for i, (bx, by) in enumerate(self.snake.body):
            if i > 0:  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≥–æ–ª–æ–≤—É
                grid[1, by, bx] = 1
        
        # –û–±'—î–∫—Ç–∏
        channel_map = {
            ObjectType.APPLE: 2,
            ObjectType.GOLDEN: 3,
            ObjectType.POISON: 4,
            ObjectType.SOUR: 5,
            ObjectType.ROTTEN: 6,
        }
        
        for obj in self.objects:
            ch = channel_map[obj.object_type]
            grid[ch, obj.y, obj.x] = 1
        
        # –ü–µ—Ä–µ—à–∫–æ–¥–∏
        for obs in self.obstacles:
            grid[7, obs.y, obs.x] = 1
        
        return grid
    
    def _is_danger(self, direction: Direction) -> float:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —î –Ω–µ–±–µ–∑–ø–µ–∫–∞ –≤ –Ω–∞–ø—Ä—è–º–∫—É."""
        hx, hy = self.snake.head
        dx, dy = direction.value
        next_pos = (hx + dx, hy + dy)
        
        # –°—Ç—ñ–Ω–∞
        if not self._is_valid_position(next_pos):
            return 1.0
        
        # –¢—ñ–ª–æ
        if next_pos in self.snake.get_body_set():
            return 1.0
        
        # –ü–µ—Ä–µ—à–∫–æ–¥–∞
        if next_pos in {obs.position for obs in self.obstacles}:
            return 1.0
        
        return 0.0
    
    def _get_left_direction(self, d: Direction) -> Direction:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–ø—Ä—è–º–æ–∫ –ª—ñ–≤–æ—Ä—É—á –≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ."""
        return Snake.TURN_LEFT_MAP[d]
    
    def _get_right_direction(self, d: Direction) -> Direction:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–ø—Ä—è–º–æ–∫ –ø—Ä–∞–≤–æ—Ä—É—á –≤—ñ–¥ –ø–æ—Ç–æ—á–Ω–æ–≥–æ."""
        return Snake.TURN_RIGHT_MAP[d]
    
    def _get_info(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –¥–æ–¥–∞—Ç–∫–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é."""
        return {
            "score": self.score,
            "length": self.snake.length,
            "steps": self.steps,
            "obstacles_count": len(self.obstacles),
        }
    
    def render(self):
        """–†–µ–Ω–¥–µ—Ä–∏—Ç—å –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω."""
        if self.render_mode is None:
            return None
        
        if self.renderer is None:
            from .renderer import Renderer
            self.renderer = Renderer(
                grid_size=self.grid_size,
                render_mode=self.render_mode
            )
        
        return self.renderer.render(
            snake=self.snake,
            objects=self.objects,
            obstacles=self.obstacles,
            score=self.score,
            steps=self.steps
        )
    
    def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ."""
        if self.renderer:
            self.renderer.close()
```

### 1.4 renderer.py

```python
"""
Pygame —Ä–µ–Ω–¥–µ—Ä–µ—Ä –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –≥—Ä–∏.
"""

import pygame
import numpy as np
from typing import Tuple, List, Optional

from .snake import Snake
from .game_objects import GameObject, ObjectType

class Renderer:
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≥—Ä–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Pygame."""
    
    # –ö–æ–ª—å–æ—Ä–∏
    COLORS = {
        "background": (30, 30, 40),
        "grid": (50, 50, 60),
        "snake_head": (0, 200, 100),
        "snake_body": (0, 150, 80),
        "apple": (220, 50, 50),
        "golden": (255, 215, 0),
        "poison": (20, 20, 20),
        "poison_skull": (200, 200, 200),
        "sour": (255, 255, 100),
        "rotten": (139, 90, 43),
        "obstacle": (100, 100, 120),
        "text": (255, 255, 255),
    }
    
    def __init__(
        self,
        grid_size: Tuple[int, int],
        cell_size: int = 30,
        render_mode: str = "human"
    ):
        """
        Args:
            grid_size: —Ä–æ–∑–º—ñ—Ä –ø–æ–ª—è (width, height)
            cell_size: —Ä–æ–∑–º—ñ—Ä –∫–ª—ñ—Ç–∏–Ω–∫–∏ –≤ –ø—ñ–∫—Å–µ–ª—è—Ö
            render_mode: "human" –∞–±–æ "rgb_array"
        """
        self.grid_size = grid_size
        self.cell_size = cell_size
        self.render_mode = render_mode
        
        # –†–æ–∑–º—ñ—Ä–∏ –≤—ñ–∫–Ω–∞
        self.window_width = grid_size[0] * cell_size
        self.window_height = grid_size[1] * cell_size + 50  # +50 –¥–ª—è —ñ–Ω—Ñ–æ-–ø–∞–Ω–µ–ª—ñ
        
        pygame.init()
        pygame.display.set_caption("Snake+ RL")
        
        if render_mode == "human":
            self.screen = pygame.display.set_mode(
                (self.window_width, self.window_height)
            )
        else:
            self.screen = pygame.Surface(
                (self.window_width, self.window_height)
            )
        
        self.font = pygame.font.Font(None, 24)
        self.clock = pygame.time.Clock()
    
    def render(
        self,
        snake: Snake,
        objects: List[GameObject],
        obstacles: List[GameObject],
        score: int,
        steps: int
    ) -> Optional[np.ndarray]:
        """
        –†–µ–Ω–¥–µ—Ä–∏—Ç—å –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω –≥—Ä–∏.
        
        Returns:
            RGB array —è–∫—â–æ render_mode == "rgb_array", —ñ–Ω–∞–∫—à–µ None
        """
        # –û—á–∏—â–∞—î–º–æ –µ–∫—Ä–∞–Ω
        self.screen.fill(self.COLORS["background"])
        
        # –ú–∞–ª—é—î–º–æ —Å—ñ—Ç–∫—É
        self._draw_grid()
        
        # –ú–∞–ª—é—î–º–æ –ø–µ—Ä–µ—à–∫–æ–¥–∏
        for obs in obstacles:
            self._draw_cell(obs.x, obs.y, self.COLORS["obstacle"])
        
        # –ú–∞–ª—é—î–º–æ –æ–±'—î–∫—Ç–∏
        for obj in objects:
            self._draw_object(obj)
        
        # –ú–∞–ª—é—î–º–æ –∑–º—ñ–π–∫—É
        self._draw_snake(snake)
        
        # –ú–∞–ª—é—î–º–æ —ñ–Ω—Ñ–æ-–ø–∞–Ω–µ–ª—å
        self._draw_info(score, steps, snake.length)
        
        if self.render_mode == "human":
            pygame.display.flip()
            self.clock.tick(10)  # 10 FPS
            return None
        else:
            return np.transpose(
                pygame.surfarray.array3d(self.screen),
                (1, 0, 2)
            )
    
    def _draw_grid(self):
        """–ú–∞–ª—é—î —Å—ñ—Ç–∫—É."""
        for x in range(self.grid_size[0] + 1):
            pygame.draw.line(
                self.screen,
                self.COLORS["grid"],
                (x * self.cell_size, 0),
                (x * self.cell_size, self.grid_size[1] * self.cell_size)
            )
        
        for y in range(self.grid_size[1] + 1):
            pygame.draw.line(
                self.screen,
                self.COLORS["grid"],
                (0, y * self.cell_size),
                (self.window_width, y * self.cell_size)
            )
    
    def _draw_cell(self, x: int, y: int, color: Tuple[int, int, int], 
                   margin: int = 2):
        """–ú–∞–ª—é—î –∑–∞–ø–æ–≤–Ω–µ–Ω—É –∫–ª—ñ—Ç–∏–Ω–∫—É."""
        rect = pygame.Rect(
            x * self.cell_size + margin,
            y * self.cell_size + margin,
            self.cell_size - 2 * margin,
            self.cell_size - 2 * margin
        )
        pygame.draw.rect(self.screen, color, rect, border_radius=5)
    
    def _draw_snake(self, snake: Snake):
        """–ú–∞–ª—é—î –∑–º—ñ–π–∫—É."""
        # –¢—ñ–ª–æ
        for i, (x, y) in enumerate(snake.body):
            if i == 0:
                # –ì–æ–ª–æ–≤–∞
                self._draw_cell(x, y, self.COLORS["snake_head"])
                # –û—á—ñ
                self._draw_eyes(x, y, snake.direction)
            else:
                # –¢—ñ–ª–æ –∑ –≥—Ä–∞–¥—ñ—î–Ω—Ç–æ–º
                ratio = i / len(snake.body)
                color = self._interpolate_color(
                    self.COLORS["snake_body"],
                    (0, 100, 50),
                    ratio
                )
                self._draw_cell(x, y, color)
    
    def _draw_eyes(self, x: int, y: int, direction):
        """–ú–∞–ª—é—î –æ—á—ñ –∑–º—ñ–π–∫–∏."""
        from .snake import Direction
        
        cx = x * self.cell_size + self.cell_size // 2
        cy = y * self.cell_size + self.cell_size // 2
        
        eye_offsets = {
            Direction.UP: [(-5, -3), (5, -3)],
            Direction.DOWN: [(-5, 3), (5, 3)],
            Direction.LEFT: [(-3, -5), (-3, 5)],
            Direction.RIGHT: [(3, -5), (3, 5)],
        }
        
        for ox, oy in eye_offsets[direction]:
            pygame.draw.circle(
                self.screen,
                (255, 255, 255),
                (cx + ox, cy + oy),
                3
            )
            pygame.draw.circle(
                self.screen,
                (0, 0, 0),
                (cx + ox, cy + oy),
                1
            )
    
    def _draw_object(self, obj: GameObject):
        """–ú–∞–ª—é—î —ñ–≥—Ä–æ–≤–∏–π –æ–±'—î–∫—Ç."""
        x, y = obj.x, obj.y
        
        color_map = {
            ObjectType.APPLE: self.COLORS["apple"],
            ObjectType.GOLDEN: self.COLORS["golden"],
            ObjectType.POISON: self.COLORS["poison"],
            ObjectType.SOUR: self.COLORS["sour"],
            ObjectType.ROTTEN: self.COLORS["rotten"],
        }
        
        color = color_map.get(obj.object_type, (255, 255, 255))
        
        # –ú–∞–ª—é—î–º–æ –∫—Ä—É–≥ –¥–ª—è —è–±–ª—É–∫
        cx = x * self.cell_size + self.cell_size // 2
        cy = y * self.cell_size + self.cell_size // 2
        radius = self.cell_size // 2 - 4
        
        pygame.draw.circle(self.screen, color, (cx, cy), radius)
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–µ—Ç–∞–ª—ñ –¥–ª—è –æ—Ç—Ä—É—Ç–∏ (—á–µ—Ä–µ–ø)
        if obj.object_type == ObjectType.POISON:
            # –ü—Ä–æ—Å—Ç–∏–π —á–µ—Ä–µ–ø
            pygame.draw.circle(
                self.screen,
                self.COLORS["poison_skull"],
                (cx, cy - 2),
                radius // 2
            )
            # –û—á—ñ
            pygame.draw.circle(self.screen, (0, 0, 0), (cx - 3, cy - 3), 2)
            pygame.draw.circle(self.screen, (0, 0, 0), (cx + 3, cy - 3), 2)
        
        # –ë–ª–∏—Å–∫ –¥–ª—è –∑–æ–ª–æ—Ç–æ–≥–æ
        if obj.object_type == ObjectType.GOLDEN:
            pygame.draw.circle(
                self.screen,
                (255, 255, 200),
                (cx - 3, cy - 3),
                3
            )
    
    def _draw_info(self, score: int, steps: int, length: int):
        """–ú–∞–ª—é—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω—É –ø–∞–Ω–µ–ª—å."""
        y = self.grid_size[1] * self.cell_size + 10
        
        texts = [
            f"Score: {score}",
            f"Steps: {steps}",
            f"Length: {length}",
        ]
        
        x_positions = [10, 150, 290]
        
        for text, x in zip(texts, x_positions):
            surface = self.font.render(text, True, self.COLORS["text"])
            self.screen.blit(surface, (x, y))
    
    def _interpolate_color(
        self,
        color1: Tuple[int, int, int],
        color2: Tuple[int, int, int],
        ratio: float
    ) -> Tuple[int, int, int]:
        """–Ü–Ω—Ç–µ—Ä–ø–æ–ª—é—î –º—ñ–∂ –¥–≤–æ–º–∞ –∫–æ–ª—å–æ—Ä–∞–º–∏."""
        return tuple(
            int(c1 + (c2 - c1) * ratio)
            for c1, c2 in zip(color1, color2)
        )
    
    def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î pygame."""
        pygame.quit()
```

---

## ü§ñ –ß–ê–°–¢–ò–ù–ê 2: –ê–≥–µ–Ω—Ç–∏ (agent/)

### 2.1 q_table_agent.py

```python
"""
–¢–∞–±–ª–∏—á–Ω–∏–π Q-learning –∞–≥–µ–Ω—Ç.
"""

import numpy as np
from typing import Dict, Tuple, Optional
import pickle
from collections import defaultdict

class QTableAgent:
    """
    Q-learning –∞–≥–µ–Ω—Ç –∑ —Ç–∞–±–ª–∏—Ü–µ—é Q-–∑–Ω–∞—á–µ–Ω—å.
    
    –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤ (features).
    """
    
    def __init__(
        self,
        n_actions: int = 3,
        learning_rate: float = 0.1,
        discount_factor: float = 0.99,
        epsilon_start: float = 1.0,
        epsilon_end: float = 0.01,
        epsilon_decay: float = 0.9995,
    ):
        """
        Args:
            n_actions: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥—ñ–π
            learning_rate: —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (Œ±)
            discount_factor: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç—É–≤–∞–Ω–Ω—è (Œ≥)
            epsilon_start: –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è Œµ
            epsilon_end: –º—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è Œµ
            epsilon_decay: —à–≤–∏–¥–∫—ñ—Å—Ç—å –∑–º–µ–Ω—à–µ–Ω–Ω—è Œµ
        """
        self.n_actions = n_actions
        self.lr = learning_rate
        self.gamma = discount_factor
        
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Q-—Ç–∞–±–ª–∏—Ü—è —è–∫ defaultdict
        self.q_table: Dict[Tuple, np.ndarray] = defaultdict(
            lambda: np.zeros(n_actions)
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.training_steps = 0
    
    def discretize_state(self, observation: np.ndarray) -> Tuple:
        """
        –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω–µ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω–∏–π –∫–ª—é—á.
        
        –î–ª—è feature observation (18 –∑–Ω–∞—á–µ–Ω—å 0-1):
        - –ë—ñ–Ω–∞—Ä–∏–∑—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è > 0.5
        """
        # –ë—ñ–Ω–∞—Ä–∏–∑–∞—Ü—ñ—è
        discrete = tuple((observation > 0.5).astype(int))
        return discrete
    
    def select_action(self, observation: np.ndarray, training: bool = True) -> int:
        """
        –í–∏–±–∏—Ä–∞—î –¥—ñ—é –∑–∞ Œµ-greedy —Å—Ç—Ä–∞—Ç–µ–≥—ñ—î—é.
        
        Args:
            observation: —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è
            training: —á–∏ –≤ —Ä–µ–∂–∏–º—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
        
        Returns:
            –Ü–Ω–¥–µ–∫—Å –¥—ñ—ó
        """
        state = self.discretize_state(observation)
        
        # Œµ-greedy
        if training and np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)
        
        # Greedy
        q_values = self.q_table[state]
        
        # –Ø–∫—â–æ –≤—Å—ñ Q-–∑–Ω–∞—á–µ–Ω–Ω—è –æ–¥–Ω–∞–∫–æ–≤—ñ, –≤–∏–±–∏—Ä–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤–æ
        if np.allclose(q_values, q_values[0]):
            return np.random.randint(self.n_actions)
        
        return np.argmax(q_values)
    
    def update(
        self,
        observation: np.ndarray,
        action: int,
        reward: float,
        next_observation: np.ndarray,
        done: bool
    ) -> float:
        """
        –û–Ω–æ–≤–ª—é—î Q-—Ç–∞–±–ª–∏—Ü—é.
        
        Q(s, a) ‚Üê Q(s, a) + Œ± * [r + Œ≥ * max_a' Q(s', a') - Q(s, a)]
        
        Returns:
            TD error
        """
        state = self.discretize_state(observation)
        next_state = self.discretize_state(next_observation)
        
        # –ü–æ—Ç–æ—á–Ω–µ Q-–∑–Ω–∞—á–µ–Ω–Ω—è
        current_q = self.q_table[state][action]
        
        # –¶—ñ–ª—å–æ–≤–µ Q-–∑–Ω–∞—á–µ–Ω–Ω—è
        if done:
            target_q = reward
        else:
            target_q = reward + self.gamma * np.max(self.q_table[next_state])
        
        # TD error
        td_error = target_q - current_q
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è
        self.q_table[state][action] += self.lr * td_error
        
        # Decay epsilon
        self.epsilon = max(
            self.epsilon_end,
            self.epsilon * self.epsilon_decay
        )
        
        self.training_steps += 1
        
        return td_error
    
    def save(self, path: str):
        """–ó–±–µ—Ä—ñ–≥–∞—î –∞–≥–µ–Ω—Ç–∞."""
        data = {
            "q_table": dict(self.q_table),
            "epsilon": self.epsilon,
            "training_steps": self.training_steps,
            "params": {
                "n_actions": self.n_actions,
                "lr": self.lr,
                "gamma": self.gamma,
            }
        }
        with open(path, "wb") as f:
            pickle.dump(data, f)
    
    def load(self, path: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∞–≥–µ–Ω—Ç–∞."""
        with open(path, "rb") as f:
            data = pickle.load(f)
        
        self.q_table = defaultdict(
            lambda: np.zeros(self.n_actions),
            data["q_table"]
        )
        self.epsilon = data["epsilon"]
        self.training_steps = data["training_steps"]
    
    def get_stats(self) -> Dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        return {
            "q_table_size": len(self.q_table),
            "epsilon": self.epsilon,
            "training_steps": self.training_steps,
        }
```

### 2.2 replay_buffer.py

```python
"""
Experience Replay –±—É—Ñ–µ—Ä –¥–ª—è DQN.
"""

import numpy as np
from collections import deque
from typing import Tuple, List
import random

class ReplayBuffer:
    """Circular buffer –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–æ—Å–≤—ñ–¥—É."""
    
    def __init__(self, capacity: int = 100000):
        """
        Args:
            capacity: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –±—É—Ñ–µ—Ä–∞
        """
        self.buffer = deque(maxlen=capacity)
    
    def push(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray,
        done: bool
    ):
        """–î–æ–¥–∞—î –¥–æ—Å–≤—ñ–¥ –¥–æ –±—É—Ñ–µ—Ä–∞."""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:
        """
        –í–∏–ø–∞–¥–∫–æ–≤–æ –≤–∏–±–∏—Ä–∞—î batch_size –µ–ª–µ–º–µ–Ω—Ç—ñ–≤.
        
        Returns:
            (states, actions, rewards, next_states, dones)
        """
        batch = random.sample(self.buffer, batch_size)
        
        states = np.array([e[0] for e in batch])
        actions = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self) -> int:
        return len(self.buffer)


class PrioritizedReplayBuffer:
    """
    Prioritized Experience Replay.
    –î–æ—Å–≤—ñ–¥ –∑ –±—ñ–ª—å—à–∏–º TD error —Å–µ–º–ø–ª—é—î—Ç—å—Å—è —á–∞—Å—Ç—ñ—à–µ.
    """
    
    def __init__(
        self,
        capacity: int = 100000,
        alpha: float = 0.6,
        beta_start: float = 0.4,
        beta_frames: int = 100000
    ):
        """
        Args:
            capacity: —Ä–æ–∑–º—ñ—Ä –±—É—Ñ–µ—Ä–∞
            alpha: —Å—Ç—É–ø—ñ–Ω—å –ø—Ä—ñ–æ—Ä–∏—Ç–µ–∑–∞—Ü—ñ—ó (0 = uniform, 1 = full priority)
            beta_start: –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è Œ≤ –¥–ª—è importance sampling
            beta_frames: —Å–∫—ñ–ª—å–∫–∏ –∫—Ä–æ–∫—ñ–≤ –¥–æ Œ≤ = 1
        """
        self.capacity = capacity
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        
        self.buffer = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
        self.frame = 0
    
    def push(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray,
        done: bool
    ):
        """–î–æ–¥–∞—î –¥–æ—Å–≤—ñ–¥ –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–º –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º."""
        max_priority = self.priorities.max() if self.buffer else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.position] = (state, action, reward, next_state, done)
        
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:
        """–°–µ–º–ø–ª—é—î –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—ñ–≤."""
        self.frame += 1
        
        # –û–±—á–∏—Å–ª—é—î–º–æ Œ≤
        beta = min(1.0, self.beta_start + 
                   self.frame * (1.0 - self.beta_start) / self.beta_frames)
        
        # –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
        priorities = self.priorities[:len(self.buffer)]
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # –°–µ–º–ø–ª—é—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # Importance sampling weights
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()
        
        # –ó–±–∏—Ä–∞—î–º–æ batch
        batch = [self.buffer[i] for i in indices]
        
        states = np.array([e[0] for e in batch])
        actions = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])
        
        return states, actions, rewards, next_states, dones, indices, weights
    
    def update_priorities(self, indices: np.ndarray, td_errors: np.ndarray):
        """–û–Ω–æ–≤–ª—é—î –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏."""
        for idx, td_error in zip(indices, td_errors):
            self.priorities[idx] = abs(td_error) + 1e-6
    
    def __len__(self) -> int:
        return len(self.buffer)
```

### 2.3 networks.py

```python
"""
–ù–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ –¥–ª—è DQN.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple

class DQN_MLP(nn.Module):
    """
    –ë–∞–≥–∞—Ç–æ—à–∞—Ä–æ–≤–∏–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω –¥–ª—è feature-based —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å.
    """
    
    def __init__(
        self,
        input_size: int = 18,
        hidden_sizes: Tuple[int, ...] = (128, 128),
        n_actions: int = 3
    ):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, n_actions))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


class DQN_CNN(nn.Module):
    """
    CNN –¥–ª—è grid-based —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å.
    """
    
    def __init__(
        self,
        input_channels: int = 8,
        grid_size: Tuple[int, int] = (15, 15),
        n_actions: int = 3
    ):
        super().__init__()
        
        self.conv = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
        )
        
        # –û–±—á–∏—Å–ª—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –ø—ñ—Å–ª—è conv
        conv_out_size = self._get_conv_output_size(input_channels, grid_size)
        
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(conv_out_size, 256),
            nn.ReLU(),
            nn.Linear(256, n_actions)
        )
    
    def _get_conv_output_size(self, channels: int, grid_size: Tuple[int, int]) -> int:
        dummy = torch.zeros(1, channels, grid_size[1], grid_size[0])
        out = self.conv(dummy)
        return int(np.prod(out.shape[1:]))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv(x)
        return self.fc(x)


class DuelingDQN(nn.Module):
    """
    Dueling DQN –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞.
    Q(s, a) = V(s) + A(s, a) - mean(A(s, a'))
    """
    
    def __init__(
        self,
        input_size: int = 18,
        hidden_size: int = 128,
        n_actions: int = 3
    ):
        super().__init__()
        
        self.feature = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU()
        )
        
        # Value stream
        self.value = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
        
        # Advantage stream
        self.advantage = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, n_actions)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.feature(x)
        
        value = self.value(features)
        advantage = self.advantage(features)
        
        # Q = V + A - mean(A)
        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)
        
        return q_values
```

### 2.4 dqn_agent.py

```python
"""
Deep Q-Network –∞–≥–µ–Ω—Ç.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, Optional

from .networks import DQN_MLP, DQN_CNN, DuelingDQN
from .replay_buffer import ReplayBuffer, PrioritizedReplayBuffer

class DQNAgent:
    """
    DQN –∞–≥–µ–Ω—Ç –∑ target network —Ç–∞ experience replay.
    """
    
    def __init__(
        self,
        observation_type: str = "features",
        n_actions: int = 3,
        learning_rate: float = 1e-4,
        discount_factor: float = 0.99,
        epsilon_start: float = 1.0,
        epsilon_end: float = 0.01,
        epsilon_decay_steps: int = 50000,
        buffer_size: int = 100000,
        batch_size: int = 64,
        target_update_freq: int = 1000,
        use_double_dqn: bool = True,
        use_dueling: bool = False,
        use_prioritized_replay: bool = False,
        device: str = "auto"
    ):
        """
        Args:
            observation_type: "features" –∞–±–æ "grid"
            n_actions: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥—ñ–π
            learning_rate: —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
            discount_factor: Œ≥
            epsilon_start/end: –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ Œµ-greedy
            epsilon_decay_steps: –∑–∞ —Å–∫—ñ–ª—å–∫–∏ –∫—Ä–æ–∫—ñ–≤ Œµ –ø–∞–¥–∞—î –¥–æ –º—ñ–Ω—ñ–º—É–º—É
            buffer_size: —Ä–æ–∑–º—ñ—Ä replay buffer
            batch_size: —Ä–æ–∑–º—ñ—Ä batch
            target_update_freq: —á–∞—Å—Ç–æ—Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è target network
            use_double_dqn: —á–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Double DQN
            use_dueling: —á–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Dueling –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É
            use_prioritized_replay: —á–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ PER
            device: "cpu", "cuda", –∞–±–æ "auto"
        """
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        self.n_actions = n_actions
        self.gamma = discount_factor
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        self.use_double_dqn = use_double_dqn
        
        # Epsilon scheduling
        self.epsilon = epsilon_start
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay_steps = epsilon_decay_steps
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–µ—Ä–µ–∂—ñ
        if observation_type == "features":
            if use_dueling:
                self.q_network = DuelingDQN(n_actions=n_actions).to(self.device)
                self.target_network = DuelingDQN(n_actions=n_actions).to(self.device)
            else:
                self.q_network = DQN_MLP(n_actions=n_actions).to(self.device)
                self.target_network = DQN_MLP(n_actions=n_actions).to(self.device)
        else:
            self.q_network = DQN_CNN(n_actions=n_actions).to(self.device)
            self.target_network = DQN_CNN(n_actions=n_actions).to(self.device)
        
        # –ö–æ–ø—ñ—é—î–º–æ –≤–∞–≥–∏
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # Replay buffer
        if use_prioritized_replay:
            self.replay_buffer = PrioritizedReplayBuffer(capacity=buffer_size)
        else:
            self.replay_buffer = ReplayBuffer(capacity=buffer_size)
        
        self.use_prioritized_replay = use_prioritized_replay
        
        # –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏
        self.training_steps = 0
        self.updates = 0
    
    def select_action(self, observation: np.ndarray, training: bool = True) -> int:
        """–í–∏–±–∏—Ä–∞—î –¥—ñ—é –∑–∞ Œµ-greedy."""
        if training and np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)
        
        with torch.no_grad():
            state = torch.FloatTensor(observation).unsqueeze(0).to(self.device)
            q_values = self.q_network(state)
            return q_values.argmax(dim=1).item()
    
    def store_transition(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray,
        done: bool
    ):
        """–ó–±–µ—Ä—ñ–≥–∞—î –ø–µ—Ä–µ—Ö—ñ–¥ –≤ –±—É—Ñ–µ—Ä."""
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def train_step(self) -> Optional[Dict]:
        """
        –í–∏–∫–æ–Ω—É—î –æ–¥–∏–Ω –∫—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è.
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–±–æ None —è–∫—â–æ –±—É—Ñ–µ—Ä –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–∏–π
        """
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # –°–µ–º–ø–ª—é—î–º–æ batch
        if self.use_prioritized_replay:
            states, actions, rewards, next_states, dones, indices, weights = \
                self.replay_buffer.sample(self.batch_size)
            weights = torch.FloatTensor(weights).to(self.device)
        else:
            states, actions, rewards, next_states, dones = \
                self.replay_buffer.sample(self.batch_size)
            weights = torch.ones(self.batch_size).to(self.device)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ —Ç–µ–Ω–∑–æ—Ä–∏
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # –ü–æ—Ç–æ—á–Ω—ñ Q-–∑–Ω–∞—á–µ–Ω–Ω—è
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # –¶—ñ–ª—å–æ–≤—ñ Q-–∑–Ω–∞—á–µ–Ω–Ω—è
        with torch.no_grad():
            if self.use_double_dqn:
                # Double DQN: –≤–∏–±–∏—Ä–∞—î–º–æ –¥—ñ—é –∑ q_network, –æ—Ü—ñ–Ω—é—î–º–æ –∑ target
                next_actions = self.q_network(next_states).argmax(dim=1)
                next_q = self.target_network(next_states).gather(
                    1, next_actions.unsqueeze(1)
                ).squeeze(1)
            else:
                next_q = self.target_network(next_states).max(dim=1)[0]
            
            target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # TD error
        td_errors = target_q - current_q
        
        # Loss (–∑–≤–∞–∂–µ–Ω–∏–π –¥–ª—è PER)
        loss = (weights * td_errors.pow(2)).mean()
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –º–µ—Ä–µ–∂—É
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)
        
        self.optimizer.step()
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏ –≤ PER
        if self.use_prioritized_replay:
            self.replay_buffer.update_priorities(
                indices,
                td_errors.detach().cpu().numpy()
            )
        
        # –û–Ω–æ–≤–ª—é—î–º–æ epsilon
        self._update_epsilon()
        
        # –û–Ω–æ–≤–ª—é—î–º–æ target network
        self.updates += 1
        if self.updates % self.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        self.training_steps += 1
        
        return {
            "loss": loss.item(),
            "mean_q": current_q.mean().item(),
            "epsilon": self.epsilon,
        }
    
    def _update_epsilon(self):
        """–û–Ω–æ–≤–ª—é—î epsilon –∑–∞ –ª—ñ–Ω—ñ–π–Ω–∏–º —Ä–æ–∑–∫–ª–∞–¥–æ–º."""
        progress = min(1.0, self.training_steps / self.epsilon_decay_steps)
        self.epsilon = self.epsilon_start + progress * (self.epsilon_end - self.epsilon_start)
    
    def save(self, path: str):
        """–ó–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å."""
        torch.save({
            "q_network": self.q_network.state_dict(),
            "target_network": self.target_network.state_dict(),
            "optimizer": self.optimizer.state_dict(),
            "epsilon": self.epsilon,
            "training_steps": self.training_steps,
            "updates": self.updates,
        }, path)
    
    def load(self, path: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å."""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.q_network.load_state_dict(checkpoint["q_network"])
        self.target_network.load_state_dict(checkpoint["target_network"])
        self.optimizer.load_state_dict(checkpoint["optimizer"])
        self.epsilon = checkpoint["epsilon"]
        self.training_steps = checkpoint["training_steps"]
        self.updates = checkpoint["updates"]
```

---

## üèãÔ∏è –ß–ê–°–¢–ò–ù–ê 3: –ù–∞–≤—á–∞–Ω–Ω—è (training/)

### 3.1 train_dqn.py

```python
"""
–°–∫—Ä–∏–ø—Ç –Ω–∞–≤—á–∞–Ω–Ω—è DQN –∞–≥–µ–Ω—Ç–∞.
"""

import argparse
import yaml
import numpy as np
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt

# –Ü–º–ø–æ—Ä—Ç–∏ –ø—Ä–æ–µ–∫—Ç—É
import sys
sys.path.append(str(Path(__file__).parent.parent))

from env.snake_env import SnakePlusEnv
from agent.dqn_agent import DQNAgent

def train(config: dict):
    """–û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è."""
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = Path(f"results/runs/{timestamp}")
    run_dir.mkdir(parents=True, exist_ok=True)
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–Ω—Ñ—ñ–≥
    with open(run_dir / "config.yaml", "w") as f:
        yaml.dump(config, f)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
    env = SnakePlusEnv(
        grid_size=tuple(config["env"]["grid_size"]),
        spawn_probs=config["env"]["spawn_probs"],
        max_objects=config["env"]["max_objects"],
        obstacle_decay=config["env"].get("obstacle_decay"),
        max_steps=config["env"]["max_steps"],
        observation_type=config["env"]["observation_type"],
    )
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–≥–µ–Ω—Ç–∞
    agent = DQNAgent(
        observation_type=config["env"]["observation_type"],
        n_actions=3,
        learning_rate=config["agent"]["learning_rate"],
        discount_factor=config["agent"]["discount_factor"],
        epsilon_start=config["agent"]["epsilon_start"],
        epsilon_end=config["agent"]["epsilon_end"],
        epsilon_decay_steps=config["agent"]["epsilon_decay_steps"],
        buffer_size=config["agent"]["buffer_size"],
        batch_size=config["agent"]["batch_size"],
        target_update_freq=config["agent"]["target_update_freq"],
        use_double_dqn=config["agent"]["use_double_dqn"],
        use_dueling=config["agent"]["use_dueling"],
        use_prioritized_replay=config["agent"]["use_prioritized_replay"],
    )
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    episode_rewards = []
    episode_lengths = []
    episode_scores = []
    losses = []
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    n_episodes = config["training"]["n_episodes"]
    eval_freq = config["training"]["eval_freq"]
    save_freq = config["training"]["save_freq"]
    
    # –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª
    for episode in tqdm(range(n_episodes), desc="Training"):
        state, info = env.reset()
        episode_reward = 0
        episode_length = 0
        
        done = False
        while not done:
            # –í–∏–±–∏—Ä–∞—î–º–æ –¥—ñ—é
            action = agent.select_action(state, training=True)
            
            # –í–∏–∫–æ–Ω—É—î–º–æ –∫—Ä–æ–∫
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–µ—Ä–µ—Ö—ñ–¥
            agent.store_transition(state, action, reward, next_state, done)
            
            # –ù–∞–≤—á–∞—î–º–æ –∞–≥–µ–Ω—Ç–∞
            metrics = agent.train_step()
            if metrics:
                losses.append(metrics["loss"])
            
            # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            state = next_state
            episode_reward += reward
            episode_length += 1
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –µ–ø—ñ–∑–æ–¥—É
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        episode_scores.append(info["score"])
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_score = np.mean(episode_scores[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            
            print(f"\nEpisode {episode + 1}")
            print(f"  Avg Reward: {avg_reward:.2f}")
            print(f"  Avg Score: {avg_score:.2f}")
            print(f"  Avg Length: {avg_length:.2f}")
            print(f"  Epsilon: {agent.epsilon:.3f}")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–æ–¥–µ–ª—å
        if (episode + 1) % save_freq == 0:
            agent.save(run_dir / f"model_ep{episode + 1}.pt")
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å
    agent.save(run_dir / "model_final.pt")
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
    np.savez(
        run_dir / "metrics.npz",
        rewards=episode_rewards,
        lengths=episode_lengths,
        scores=episode_scores,
        losses=losses,
    )
    
    # –ë—É–¥—É—î–º–æ –≥—Ä–∞—Ñ—ñ–∫–∏
    plot_training_curves(episode_rewards, episode_scores, losses, run_dir)
    
    env.close()
    print(f"\nTraining complete! Results saved to {run_dir}")


def plot_training_curves(rewards, scores, losses, save_dir):
    """–ë—É–¥—É—î –≥—Ä–∞—Ñ—ñ–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è."""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Rewards
    axes[0, 0].plot(rewards, alpha=0.3)
    axes[0, 0].plot(moving_average(rewards, 100))
    axes[0, 0].set_title("Episode Rewards")
    axes[0, 0].set_xlabel("Episode")
    axes[0, 0].set_ylabel("Reward")
    
    # Scores
    axes[0, 1].plot(scores, alpha=0.3)
    axes[0, 1].plot(moving_average(scores, 100))
    axes[0, 1].set_title("Episode Scores")
    axes[0, 1].set_xlabel("Episode")
    axes[0, 1].set_ylabel("Score")
    
    # Losses
    axes[1, 0].plot(losses, alpha=0.3)
    axes[1, 0].plot(moving_average(losses, 1000))
    axes[1, 0].set_title("Training Loss")
    axes[1, 0].set_xlabel("Step")
    axes[1, 0].set_ylabel("Loss")
    
    # Histogram of final rewards
    axes[1, 1].hist(rewards[-1000:], bins=50)
    axes[1, 1].set_title("Reward Distribution (last 1000)")
    axes[1, 1].set_xlabel("Reward")
    axes[1, 1].set_ylabel("Count")
    
    plt.tight_layout()
    plt.savefig(save_dir / "training_curves.png", dpi=150)
    plt.close()


def moving_average(data, window):
    """–û–±—á–∏—Å–ª—é—î –∫–æ–≤–∑–Ω–µ —Å–µ—Ä–µ–¥–Ω—î."""
    return np.convolve(data, np.ones(window) / window, mode='valid')


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/training.yaml")
    args = parser.parse_args()
    
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    train(config)
```

---

## üî¨ –ß–ê–°–¢–ò–ù–ê 4: –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏ (experiments/)

### 4.1 discount_analysis.py

```python
"""
–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞ –¥–∏—Å–∫–æ–Ω—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é.
"""

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import yaml
from typing import List, Dict

# –Ü–º–ø–æ—Ä—Ç–∏ –ø—Ä–æ–µ–∫—Ç—É
import sys
sys.path.append(str(Path(__file__).parent.parent))

from env.snake_env import SnakePlusEnv
from agent.dqn_agent import DQNAgent
from training.train_dqn import train

def run_discount_experiment(
    gamma_values: List[float] = [0.1, 0.5, 0.9, 0.99, 0.999],
    n_episodes: int = 5000,
    n_eval_episodes: int = 100,
    base_config_path: str = "configs/training.yaml"
):
    """
    –ó–∞–ø—É—Å–∫–∞—î –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑ —Ä—ñ–∑–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ Œ≥.
    
    –ì—ñ–ø–æ—Ç–µ–∑–∞:
    - –ù–∏–∑—å–∫–∏–π Œ≥ (0.1-0.5): –∞–≥–µ–Ω—Ç —É–Ω–∏–∫–∞—î –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è,
      —á–∞—Å—Ç—ñ—à–µ —ó—Å—Ç—å –≥–Ω–∏–ª—ñ —è–±–ª—É–∫–∞, —à–≤–∏–¥–∫–æ –≥–∏–Ω–µ –≤—ñ–¥ –ø–µ—Ä–µ—à–∫–æ–¥
    - –í–∏—Å–æ–∫–∏–π Œ≥ (0.9-0.999): –∞–≥–µ–Ω—Ç —É–Ω–∏–∫–∞—î –≥–Ω–∏–ª–∏—Ö —è–±–ª—É–∫,
      –¥–æ–≤—à–µ –≤–∏–∂–∏–≤–∞—î, –Ω–∞–∫–æ–ø–∏—á—É—î –±—ñ–ª—å—à–µ –æ—á–æ–∫
    """
    results = {}
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤–∏–π –∫–æ–Ω—Ñ—ñ–≥
    with open(base_config_path) as f:
        base_config = yaml.safe_load(f)
    
    for gamma in gamma_values:
        print(f"\n{'='*50}")
        print(f"Training with Œ≥ = {gamma}")
        print(f"{'='*50}")
        
        # –ú–æ–¥–∏—Ñ—ñ–∫—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥
        config = base_config.copy()
        config["agent"]["discount_factor"] = gamma
        config["training"]["n_episodes"] = n_episodes
        
        # –ù–∞–≤—á–∞—î–º–æ –∞–≥–µ–Ω—Ç–∞
        agent, env = train_and_return(config)
        
        # –û—Ü—ñ–Ω—é—î–º–æ
        eval_results = evaluate_agent(agent, env, n_eval_episodes)
        
        results[gamma] = {
            "mean_score": np.mean(eval_results["scores"]),
            "mean_length": np.mean(eval_results["lengths"]),
            "mean_steps": np.mean(eval_results["steps"]),
            "rotten_eaten": np.mean(eval_results["rotten_eaten"]),
            "death_by_obstacle": np.mean(eval_results["death_by_obstacle"]),
            "survival_rate": np.mean(eval_results["survived"]),
        }
        
        print(f"Results for Œ≥ = {gamma}:")
        for k, v in results[gamma].items():
            print(f"  {k}: {v:.3f}")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plot_discount_analysis(results)
    
    return results


def evaluate_agent(
    agent: DQNAgent,
    env: SnakePlusEnv,
    n_episodes: int
) -> Dict[str, List]:
    """–û—Ü—ñ–Ω—é—î –∞–≥–µ–Ω—Ç–∞."""
    results = {
        "scores": [],
        "lengths": [],
        "steps": [],
        "rotten_eaten": [],
        "death_by_obstacle": [],
        "survived": [],
    }
    
    for _ in range(n_episodes):
        state, info = env.reset()
        done = False
        rotten_count = 0
        
        while not done:
            action = agent.select_action(state, training=False)
            state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # –†–∞—Ö—É—î–º–æ –≥–Ω–∏–ª—ñ —è–±–ª—É–∫–∞ (–∑–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–æ—é)
            if reward == -20:  # ROTTEN reward
                rotten_count += 1
        
        results["scores"].append(info["score"])
        results["lengths"].append(info["length"])
        results["steps"].append(info["steps"])
        results["rotten_eaten"].append(rotten_count)
        results["death_by_obstacle"].append(
            1 if info["obstacles_count"] > 0 and terminated else 0
        )
        results["survived"].append(1 if truncated else 0)
    
    return results


def plot_discount_analysis(results: Dict[float, Dict]):
    """–ë—É–¥—É—î –≥—Ä–∞—Ñ—ñ–∫–∏ –∞–Ω–∞–ª—ñ–∑—É."""
    gammas = list(results.keys())
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    metrics = [
        ("mean_score", "Average Score"),
        ("mean_steps", "Average Survival Steps"),
        ("rotten_eaten", "Avg Rotten Apples Eaten"),
        ("death_by_obstacle", "Death by Obstacle Rate"),
        ("survival_rate", "Survival Rate (reached max steps)"),
        ("mean_length", "Average Final Length"),
    ]
    
    for ax, (metric, title) in zip(axes.flatten(), metrics):
        values = [results[g][metric] for g in gammas]
        ax.bar(range(len(gammas)), values, tick_label=[str(g) for g in gammas])
        ax.set_title(title)
        ax.set_xlabel("Discount Factor (Œ≥)")
        ax.set_ylabel(metric)
    
    plt.suptitle("Impact of Discount Factor on Agent Behavior", fontsize=14)
    plt.tight_layout()
    plt.savefig("results/discount_analysis.png", dpi=150)
    plt.show()


if __name__ == "__main__":
    run_discount_experiment()
```

---

## üìä –ß–ê–°–¢–ò–ù–ê 5: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (configs/)

### 5.1 default_env.yaml

```yaml
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ Snake+

grid_size: [15, 15]

spawn_probs:
  apple: 0.50
  golden: 0.10
  poison: 0.15
  sour: 0.15
  rotten: 0.10

max_objects: 5
obstacle_decay: 50  # null –¥–ª—è –≤—ñ—á–Ω–∏—Ö –ø–µ—Ä–µ—à–∫–æ–¥
max_steps: 1000
observation_type: "features"  # –∞–±–æ "grid"
```

### 5.2 training.yaml

```yaml
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è

env:
  grid_size: [15, 15]
  spawn_probs:
    apple: 0.50
    golden: 0.10
    poison: 0.15
    sour: 0.15
    rotten: 0.10
  max_objects: 5
  obstacle_decay: 50
  max_steps: 1000
  observation_type: "features"

agent:
  learning_rate: 0.0001
  discount_factor: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay_steps: 50000
  buffer_size: 100000
  batch_size: 64
  target_update_freq: 1000
  use_double_dqn: true
  use_dueling: false
  use_prioritized_replay: false

training:
  n_episodes: 10000
  eval_freq: 500
  save_freq: 1000
```

---

## üéØ –ß–ê–°–¢–ò–ù–ê 6: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è (visualization/)

### 6.1 dashboard.py

```python
"""
–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó.
"""

import pygame
import numpy as np
from pathlib import Path

# –Ü–º–ø–æ—Ä—Ç–∏ –ø—Ä–æ–µ–∫—Ç—É
import sys
sys.path.append(str(Path(__file__).parent.parent))

from env.snake_env import SnakePlusEnv
from agent.dqn_agent import DQNAgent

class Dashboard:
    """
    –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è:
    - –ì—Ä–∏ –≤—Ä—É—á–Ω—É
    - –°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞ –∞–≥–µ–Ω—Ç–æ–º
    - –ó–º—ñ–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
    """
    
    def __init__(self, config_path: str = "configs/training.yaml"):
        pygame.init()
        
        # –†–æ–∑–º—ñ—Ä–∏ –≤—ñ–∫–Ω–∞
        self.game_width = 450  # 15 * 30
        self.panel_width = 300
        self.window_width = self.game_width + self.panel_width
        self.window_height = 500
        
        self.screen = pygame.display.set_mode(
            (self.window_width, self.window_height)
        )
        pygame.display.set_caption("Snake+ RL Dashboard")
        
        # –®—Ä–∏—Ñ—Ç–∏
        self.font = pygame.font.Font(None, 24)
        self.font_large = pygame.font.Font(None, 32)
        
        # –°—Ç–∞–Ω
        self.mode = "agent"  # "agent" –∞–±–æ "human"
        self.paused = False
        self.speed = 10  # FPS
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥
        import yaml
        with open(config_path) as f:
            self.config = yaml.safe_load(f)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
        self.env = SnakePlusEnv(
            grid_size=tuple(self.config["env"]["grid_size"]),
            render_mode="rgb_array"
        )
        
        # –ê–≥–µ–Ω—Ç (–±—É–¥–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ)
        self.agent = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "episodes": 0,
            "total_score": 0,
            "best_score": 0,
            "avg_score": 0,
        }
        
        self.clock = pygame.time.Clock()
    
    def load_agent(self, model_path: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –Ω–∞–≤—á–µ–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
        self.agent = DQNAgent(
            observation_type=self.config["env"]["observation_type"],
            discount_factor=self.config["agent"]["discount_factor"],
        )
        self.agent.load(model_path)
        self.agent.epsilon = 0  # –í–∏–º–∏–∫–∞—î–º–æ exploration
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞—î –¥–∞—à–±–æ—Ä–¥."""
        state, _ = self.env.reset()
        running = True
        
        while running:
            # –û–±—Ä–æ–±–∫–∞ –ø–æ–¥—ñ–π
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_SPACE:
                        self.paused = not self.paused
                    elif event.key == pygame.K_m:
                        self.mode = "human" if self.mode == "agent" else "agent"
                    elif event.key == pygame.K_r:
                        state, _ = self.env.reset()
                    elif event.key == pygame.K_UP:
                        self.speed = min(60, self.speed + 5)
                    elif event.key == pygame.K_DOWN:
                        self.speed = max(1, self.speed - 5)
            
            if not self.paused:
                # –í–∏–±–∏—Ä–∞—î–º–æ –¥—ñ—é
                if self.mode == "agent" and self.agent:
                    action = self.agent.select_action(state, training=False)
                else:
                    # –ö–µ—Ä—É–≤–∞–Ω–Ω—è –ª—é–¥–∏–Ω–æ—é
                    keys = pygame.key.get_pressed()
                    if keys[pygame.K_LEFT]:
                        action = 1  # Turn left
                    elif keys[pygame.K_RIGHT]:
                        action = 2  # Turn right
                    else:
                        action = 0  # Forward
                
                # –ö—Ä–æ–∫
                next_state, reward, terminated, truncated, info = self.env.step(action)
                
                if terminated or truncated:
                    self.stats["episodes"] += 1
                    self.stats["total_score"] += info["score"]
                    self.stats["best_score"] = max(
                        self.stats["best_score"], 
                        info["score"]
                    )
                    self.stats["avg_score"] = (
                        self.stats["total_score"] / self.stats["episodes"]
                    )
                    state, _ = self.env.reset()
                else:
                    state = next_state
            
            # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥
            self._render(info if 'info' in dir() else {"score": 0, "length": 3})
            
            self.clock.tick(self.speed)
        
        pygame.quit()
    
    def _render(self, info: dict):
        """–†–µ–Ω–¥–µ—Ä–∏—Ç—å –≤–µ—Å—å —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å."""
        # –û—á–∏—â–∞—î–º–æ
        self.screen.fill((30, 30, 40))
        
        # –ì—Ä–∞
        game_surface = self.env.render()
        if game_surface is not None:
            game_surface = pygame.surfarray.make_surface(
                game_surface.swapaxes(0, 1)
            )
            self.screen.blit(game_surface, (0, 0))
        
        # –ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è
        self._draw_panel(info)
        
        pygame.display.flip()
    
    def _draw_panel(self, info: dict):
        """–ú–∞–ª—é—î –ø–∞–Ω–µ–ª—å –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é."""
        x = self.game_width + 10
        y = 10
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = self.font_large.render("Snake+ Dashboard", True, (255, 255, 255))
        self.screen.blit(title, (x, y))
        y += 40
        
        # –†–µ–∂–∏–º
        mode_text = f"Mode: {self.mode.upper()}"
        mode_color = (100, 200, 100) if self.mode == "agent" else (200, 200, 100)
        self.screen.blit(
            self.font.render(mode_text, True, mode_color), (x, y)
        )
        y += 30
        
        # –°—Ç–∞—Ç—É—Å
        status = "PAUSED" if self.paused else "RUNNING"
        status_color = (200, 100, 100) if self.paused else (100, 200, 100)
        self.screen.blit(
            self.font.render(f"Status: {status}", True, status_color), (x, y)
        )
        y += 30
        
        # –®–≤–∏–¥–∫—ñ—Å—Ç—å
        self.screen.blit(
            self.font.render(f"Speed: {self.speed} FPS", True, (200, 200, 200)), 
            (x, y)
        )
        y += 40
        
        # –ü–æ—Ç–æ—á–Ω–∞ –≥—Ä–∞
        self.screen.blit(
            self.font_large.render("Current Game", True, (255, 255, 255)), (x, y)
        )
        y += 30
        self.screen.blit(
            self.font.render(f"Score: {info.get('score', 0)}", True, (200, 200, 200)), 
            (x, y)
        )
        y += 25
        self.screen.blit(
            self.font.render(f"Length: {info.get('length', 3)}", True, (200, 200, 200)), 
            (x, y)
        )
        y += 40
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.screen.blit(
            self.font_large.render("Statistics", True, (255, 255, 255)), (x, y)
        )
        y += 30
        self.screen.blit(
            self.font.render(f"Episodes: {self.stats['episodes']}", True, (200, 200, 200)), 
            (x, y)
        )
        y += 25
        self.screen.blit(
            self.font.render(f"Best Score: {self.stats['best_score']}", True, (200, 200, 200)), 
            (x, y)
        )
        y += 25
        self.screen.blit(
            self.font.render(f"Avg Score: {self.stats['avg_score']:.1f}", True, (200, 200, 200)), 
            (x, y)
        )
        y += 40
        
        # –£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è
        self.screen.blit(
            self.font_large.render("Controls", True, (255, 255, 255)), (x, y)
        )
        y += 30
        controls = [
            "SPACE - Pause/Resume",
            "M - Toggle Mode",
            "R - Reset Game",
            "‚Üë/‚Üì - Speed",
            "‚Üê/‚Üí - Turn (Human)",
        ]
        for ctrl in controls:
            self.screen.blit(
                self.font.render(ctrl, True, (150, 150, 150)), (x, y)
            )
            y += 22


if __name__ == "__main__":
    dashboard = Dashboard()
    
    # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å
    import sys
    if len(sys.argv) > 1:
        dashboard.load_agent(sys.argv[1])
    
    dashboard.run()
```

---

## ‚úÖ –ß–µ–∫–ª—ñ—Å—Ç –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

### –§–∞–∑–∞ 1: –ë–∞–∑–æ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ‚úÖ
- [x] –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
- [x] –ù–∞–ø–∏—Å–∞—Ç–∏ `requirements.txt`
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `game_objects.py` ‚Äî ObjectType (6 —Ç–∏–ø—ñ–≤), GameObject, ObjectFactory, RewardCalculator
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `snake.py` ‚Äî Direction, Action, Snake –∑ deque-based body, grow/shrink/detach_tail
- [x] –ù–∞–ø–∏—Å–∞—Ç–∏ unit-—Ç–µ—Å—Ç–∏ –¥–ª—è game logic ‚Äî test_game_logic.py (24+ —Ç–µ—Å—Ç—ñ–≤)

### –§–∞–∑–∞ 2: –°–µ—Ä–µ–¥–æ–≤–∏—â–µ ‚úÖ
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `snake_env.py` ‚Äî Gymnasium env, Discrete(3) actions, feature (18-dim) —Ç–∞ grid (8√ó15√ó15) observations
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `renderer.py` ‚Äî Pygame –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∫–æ–ª—å–æ—Ä–æ–≤–∏–º–∏ –æ–±'—î–∫—Ç–∞–º–∏, info panel, human/rgb_array modes
- [x] –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º –∞–≥–µ–Ω—Ç–æ–º ‚Äî test_env.py (13+ —Ç–µ—Å—Ç—ñ–≤)
- [ ] –ó–∞—Ä–µ—î—Å—Ç—Ä—É–≤–∞—Ç–∏ –≤ Gymnasium

### –§–∞–∑–∞ 3: –ê–≥–µ–Ω—Ç–∏ ‚úÖ
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `q_table_agent.py` ‚Äî —Ç–∞–±–ª–∏—á–Ω–∏–π Q-learning, Œµ-greedy, discretization, save/load
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `replay_buffer.py` ‚Äî ReplayBuffer + PrioritizedReplayBuffer –∑ importance sampling
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `networks.py` ‚Äî DQN_MLP, DQN_CNN, DuelingDQN –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
- [x] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `dqn_agent.py` ‚Äî Double DQN, target network, PER, gradient clipping, CUDA support
- [x] –ù–∞–ø–∏—Å–∞—Ç–∏ unit-—Ç–µ—Å—Ç–∏ –¥–ª—è –∞–≥–µ–Ω—Ç—ñ–≤ ‚Äî test_agent.py (10+ —Ç–µ—Å—Ç—ñ–≤)
- [x] –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ dropout –ø—ñ–¥ —á–∞—Å inference –≤ select_action()
- [x] –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ dropout –≤ train_step –¥–ª—è target network
- [x] –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ off-by-one –ø–æ–º–∏–ª–∫—É –≤ obstacle lifetime decay

### –§–∞–∑–∞ 4: –ù–∞–≤—á–∞–Ω–Ω—è
- [ ] –ù–∞–ø–∏—Å–∞—Ç–∏ `train_dqn.py`
- [ ] –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥–∏
- [ ] –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–µ—Ä—à–µ –Ω–∞–≤—á–∞–Ω–Ω—è
- [ ] –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏

### –§–∞–∑–∞ 5: –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏
- [ ] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `discount_analysis.py`
- [ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏–º–∏ Œ≥
- [ ] –ó–∞–¥–æ–∫—É–º–µ–Ω—Ç—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

### –§–∞–∑–∞ 6: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
- [ ] –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ `dashboard.py`
- [ ] –ó–∞–ø–∏—Å–∞—Ç–∏ –≤—ñ–¥–µ–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
- [x] –ù–∞–ø–∏—Å–∞—Ç–∏ README.md
- [ ] –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –∑–≤—ñ—Ç

---

## üìù –ü—Ä–∏–º—ñ—Ç–∫–∏ –¥–ª—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó

1. **–ü–æ—á–∏–Ω–∞–π –∑ —Ç–µ—Å—Ç—ñ–≤** - –Ω–∞–ø–∏—à–∏ —Ç–µ—Å—Ç–∏ –¥–ª—è game logic –ø–µ—Ä–µ–¥ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—î—é

2. **–Ü—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞ —Ä–æ–∑—Ä–æ–±–∫–∞** - —Å–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ—Å—Ç–∞ –≤–µ—Ä—Å—ñ—è, –ø–æ—Ç—ñ–º —É—Å–∫–ª–∞–¥–Ω—é–π

3. **–í–µ—Ä—Å—ñ–æ–Ω—É–≤–∞–Ω–Ω—è** - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π git, –∫–æ–º—ñ—Ç—å —á–∞—Å—Ç–æ

4. **–õ–æ–≥—É–≤–∞–Ω–Ω—è** - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π TensorBoard –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è

5. **Reproducibility** - —Ñ—ñ–∫—Å—É–π random seeds

6. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è** - docstrings –¥–ª—è –≤—Å—ñ—Ö —Ñ—É–Ω–∫—Ü—ñ–π

7. **–ü—Ä–æ—Ñ—ñ–ª—é–≤–∞–Ω–Ω—è** - —è–∫—â–æ –ø–æ–≤—ñ–ª—å–Ω–æ, –ø—Ä–æ—Ñ—ñ–ª—é–π —ñ –æ–ø—Ç–∏–º—ñ–∑—É–π
